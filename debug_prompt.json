{
  "query": "what is attention layer?",
  "num_documents": 5,
  "document_sources": [
    "data\\sample.pdf",
    "data\\sample.pdf",
    "data\\sample.pdf",
    "data\\sample.pdf",
    "data\\sample.pdf"
  ],
  "prompt": "Context:\nDocument 1 (Source: data\\sample.pdf): around each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\n\nDocument 2 (Source: data\\sample.pdf): i , V WV\ni )\nWhere the projections are parameter matricesWQ\ni \u2208 Rdmodel\u00d7dk , WK\ni \u2208 Rdmodel\u00d7dk , WV\ni \u2208 Rdmodel\u00d7dv\nand WO \u2208 Rhdv\u00d7dmodel .\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\n\nDocument 3 (Source: data\\sample.pdf): 3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\nDocument 4 (Source: data\\sample.pdf): Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n\nDocument 5 (Source: data\\sample.pdf): the maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\n\nQuestion: what is attention layer?\nAnswer:"
}